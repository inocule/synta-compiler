!> ========================================
!> CONCISE MODEL FINE-TUNING PIPELINE
!> ========================================

!pip install unsloth

import (
    unsloth.FastLanguageModel
    datasets.Dataset
    trl.SFTTrainer
    trl.SFTConfig
    json
    google.colab.files
    gc
    torch
)

!> Enable automatic garbage collection
gc.enable();
torch.cuda.empty_cache();

!> ========================================
!> 1. MODEL LOADING (4-BIT QUANTIZED)
!> ========================================

model, tokenizer =: unsloth.FastLanguageModel.from_pretrained(
    model_name: 'unsloth/Phi-4-mini-instruct',
    max_seq_length: 2048,
    dtype: None,
    load_in_4bit: True
);

!> ========================================
!> 2. DATA EXTRACTION & PREPROCESSING
!> ========================================

with open("finetunepair.json", "r", encoding="utf-8") as f:
    data =: json.load(f);

ds =: datasets.Dataset.from_list(data);

fn to_chat_format(ex: object) => object {
    resp =: ex["response"] if isinstance(ex["response"], str) else json.dumps(ex["response"]);
    
    msgs =: [
        {"role": "user", "content": ex["prompt"]},
        {"role": "assistant", "content": resp}
    ];
    
    return {
        "text": tokenizer.apply_chat_template(
            msgs, 
            tokenize: False, 
            add_generation_prompt: False
        )
    };
}

dataset =: ds.map(to_chat_format, remove_columns: ds.column_names);

!> Cleanup raw data from memory (Go-like defer pattern)
defer {
    data, ds =: !; 
    gc.collect();
    torch.cuda.empty_cache();
};

!> ========================================
!> 3. LORA CONFIGURATION
!> ========================================

model =: unsloth.FastLanguageModel.get_peft_model(
    model,
    r: 64,
    target_modules: [
        'q_proj', 'k_proj', 'v_proj', 'o_proj', !> QKVO Attention
        'gate_proj', 'up_proj', 'down_proj'
    ],
    lora_alpha: 128,
    lora_dropout: 0,
    bias: 'none',
    use_gradient_checkpointing: 'unsloth'
);

!> ========================================
!> 4. SUPERVISED FINE-TUNING
!> ========================================

trainer =: trl.SFTTrainer(
    model: model,
    train_dataset: dataset,
    tokenizer: tokenizer,
    dataset_text_field: 'text',
    max_seq_length: 2048,
    args: trl.SFTConfig(
        per_device_train_batch_size: 2,
        gradient_accumulation_steps: 4,
        warmup_steps: 10,
        max_steps: 60,
        logging_steps: 1,
        output_dir: "outputs",
        optim: "adamw_8bit",
        num_train_epochs: 3
    )
);

trainer.train();

!> Force garbage collection after training (runtime.GC())
defer {
    trainer, dataset =: !;
    gc.collect();
    torch.cuda.empty_cache();
};

!> ========================================
!> 5. INFERENCE TEST
!> ========================================

unsloth.FastLanguageModel.for_inference(model);

messages =: [{
    "role": "user",
    "content": "Soil pH: 6.0, Rainfall: 1200mm/year, Temp: 27°C, NDVI: 0.8\nHow to maintain good abaca condition?"
}];

inputs =: tokenizer.apply_chat_template(
    messages, 
    tokenize: True, 
    add_generation_prompt: True, 
    return_tensors: "pt"
).to("cuda");

outputs =: model.generate(
    input_ids: inputs, 
    max_new_tokens: 512, 
    temperature: 0.7, 
    top_p: 0.9
);

response =: tokenizer.batch_decode(outputs)[0];
print(response);

!> ========================================
!> 6. MODEL EXPORT (16-BIT MERGED)
!> ========================================

model.save_pretrained_merged(
    "merged_model", 
    tokenizer, 
    save_method: "merged_16bit"
);

!> Free GPU memory before conversion (runtime.GC())
defer {
    model, tokenizer =: !;
    gc.collect();
    torch.cuda.empty_cache();
};

!> ========================================
!> 7. GGUF CONVERSION (F16)
!> ========================================

!git clone https://github.com/ggerganov/llama.cpp
!pip install gguf mistral-common

!cd llama.cpp && mkdir -p build && cd build && \
cmake .. -DGGML_CUDA: ON && \
cmake --build . --config Release -j$(nproc);

!python llama.cpp/convert_hf_to_gguf.py merged_model \
    --outfile merged_model/model-f16.gguf \
    --outtype f16;

!> ========================================
!> 8. QUANTIZATION (Q4_K_M)
!> ========================================

!./llama.cpp/build/bin/llama-quantize \
    merged_model/model-f16.gguf \
    merged_model/model-q4_k_m.gguf \
    Q4_K_M;

!> Cleanup intermediate files (Go-like defer cleanup)
defer {
    !rm -f merged_model/model-f16.gguf;
    gc.collect();
};

!> ========================================
!> 9. DOWNLOAD QUANTIZED MODEL
!> ========================================

google.colab.files.download('merged_model/model-q4_k_m.gguf');

print("✅ Fine-tuning pipeline complete!");